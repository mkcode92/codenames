{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Codename Hints\n",
    "\n",
    "This notebook explores the strategies `EmbeddingSimilarity`, `MaskFilling` and `MaskFillingWithExclusion` and gives an intro to the general usage.\n",
    "\n",
    "Note that implementations in the notebook and the codenames module may diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Game Words\n",
    "\n",
    "We start with 25 random words from a list of ambiguous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "from codenames.model import Game\n",
    "from codenames.utils import print_game\n",
    "\n",
    "game = Game.create_random()\n",
    "print_game(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Hint words\n",
    "\n",
    "For possible hints, we want to search a big vocabulary.\n",
    "The tokenizer of BERT (a WordPiece tokenizer) contains a lot of English words as well as inflections of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, set_seed\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "\n",
    "# filter out non-word / non-English tokens and tokens contained in the game\n",
    "def wordlike(s: str) -> bool:\n",
    "    return s.isalpha() and len(s) > 2\n",
    "\n",
    "\n",
    "hint_words = {\n",
    "    t: idx for t, idx in vocab.items() if wordlike(t) and t not in game.all_words\n",
    "}\n",
    "\n",
    "print(f\"Using {len(hint_words)} hint words\")\n",
    "print(\"Examples:\", random.sample(sorted(hint_words), 20))\n",
    "print(\"Game words missing in vocab:\", [w for w in game.all_words if w not in vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "\n",
    "I do not have a dataset of useful Codenames hints, so I'll use pretrained models or unsupervised methods.\n",
    "\n",
    "Given a game like the one above, a strategy should provide hints, together with a score and the intended target words.\n",
    "\n",
    "```python\n",
    "# model.py\n",
    "\n",
    "class Hint(BaseModel):\n",
    "    word: str           # the hint word\n",
    "    score: float        # the quality of the hint\n",
    "    targets: list[str]  # the game words targeted by the hint\n",
    "\n",
    "\n",
    "# strategy.py\n",
    "\n",
    "class HintStrategy:\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, game: Game, hint_for_red=True) -> Iterator[Hint]:\n",
    "        pass\n",
    "\n",
    "    ...\n",
    "```\n",
    "\n",
    "For the evaluation, we look at the top k results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Strategy 1: Similarities in the embedding space\n",
    "\n",
    "One simple approach is just to look at pretrained **embeddings** and their similarities.\n",
    "\n",
    "Targets are selected by their similarity to the hint word.\n",
    "Then, the selection is scored, based on the categories of the selected words.\n",
    "The Baseline strategy is configurable with weights for each category.\n",
    "\n",
    "`cosine_similarity` is a common similarity measure that looks at the angles in the embedding vector space. Other similarities could be euclidean distance or dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codenames.strategy import HintStrategy\n",
    "from codenames.model import Hint, TeamWeights\n",
    "\n",
    "from typing import Iterator, Callable\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "Words2Ids = Callable[[list[str]], list[int]]\n",
    "\n",
    "\n",
    "class EmbeddingSimilarity(HintStrategy):\n",
    "    default_weights = TeamWeights(team=1, other_team=-5, neutral=-2, assassin=-10)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: nn.Embedding,\n",
    "        words2ids: Words2Ids,\n",
    "        hint_words: dict[str, int],\n",
    "        weights: TeamWeights | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param embeddings: some pretrained embeddings\n",
    "        :param words2ids: mapping game words to their embedding indices\n",
    "        :param hint_words: a mapping of words and indices for the hints\n",
    "        :param weights: factors for the incremental score calculation\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.words2ids = words2ids\n",
    "        self.hint_words = hint_words\n",
    "        self.weights = weights or self.default_weights\n",
    "\n",
    "    def __call__(self, game: Game, hint_for_red=True) -> Iterator[Hint]:\n",
    "        words = game.all_words\n",
    "        similarities = self.calculate_similarities(game)\n",
    "\n",
    "        # incrementally calculate the score\n",
    "        for hint_word, per_hint_similarities in zip(self.hint_words, similarities):\n",
    "            score = 0\n",
    "            targets = []\n",
    "            # order by high similarity\n",
    "            for similarity, word, cat in sorted(\n",
    "                zip(per_hint_similarities, words, game.teams(hint_for_red)),\n",
    "                reverse=True,\n",
    "            ):\n",
    "                targets.append(word)\n",
    "                # The score is affected by the similarity itself\n",
    "                # and the weight per category (which also affects the polarity)\n",
    "                score += similarity * getattr(self.weights, cat)\n",
    "                yield Hint(word=hint_word, score=score, targets=targets)\n",
    "\n",
    "    def calculate_similarities(self, game):\n",
    "        # calculate all similarities between hint words and game words\n",
    "        with torch.no_grad():\n",
    "            game_word_ids = self.words2ids(game.all_words)\n",
    "            game_embs = self.embeddings(\n",
    "                torch.tensor(game_word_ids)\n",
    "            )  # shape: (25, n_emb)\n",
    "            hint_word_embs = self.embeddings(\n",
    "                torch.tensor(list(self.hint_words.values()))\n",
    "            )  # shape: (n_vocab, n_emb)\n",
    "            similarities = cosine_similarity(\n",
    "                hint_word_embs, game_embs\n",
    "            )  # shape (n_vocab, n_emb)\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "\n",
    "def words2ids(words):\n",
    "    return [s[1] for s in tokenizer(words)[\"input_ids\"]]\n",
    "\n",
    "\n",
    "embeddings_strategy = EmbeddingSimilarity(embeddings, words2ids, hint_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_strategy.print_top_k(game, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_strategy.print_top_k(game, 10, hint_for_red=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "This already works much better than I had expected. In particular, I expected the polysemous nature of the game words to make this a very difficult task.\n",
    "\n",
    "All top results only target words of their own team. Changing the weights might allow neutral words to be included but therefore target more team words as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Strategy 2: MASK Filling\n",
    "\n",
    "Another approach is to use the mask filling head for BERT.\n",
    "\n",
    "For that, a subset of the game words is provided as a textual input and the model must predict a good fit.\n",
    "\n",
    "A big problem is the number of possible subsets (25^2 = 33 Mio) - however, we can exclude subsets that:\n",
    "* contain the assassin\n",
    "* more than 2 words of the wrong color\n",
    "* less than two words\n",
    "  \n",
    "An important choice is the template of the query; some possibilities:\n",
    "* `\"The words <words> are all closely related to [MASK], but not necessarily in the same way\"`\n",
    "* `\"<words> are all related by [MASK]\"`\n",
    "* `\"<words> and [MASK]\"`\n",
    "\n",
    "It is a general problem, that this strategy inverts the task.\n",
    "It does not find game words given a hint, but a hint given a subset of game words.\n",
    "A hint may be a very good fit for a subset but also for words not included in the subset - so it may hit the assassin more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "from transformers import pipeline\n",
    "from typing import Iterator, Sequence\n",
    "from scipy.special import comb\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "TEMPLATE = \"The words {} can be thought of together with the word [MASK], maybe also through different semantic aspects.\"\n",
    "SubsetGenerator = Iterator[list[str]]\n",
    "\n",
    "\n",
    "class MaskFilling(HintStrategy):\n",
    "    default_weights = TeamWeights(team=1, other_team=-3, neutral=-1, assassin=-100)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        weights: TeamWeights | None = None,\n",
    "        max_good_targets: int = 4,\n",
    "        max_bad_targets: int = 1,\n",
    "        template=TEMPLATE,\n",
    "    ):\n",
    "        self.unmasker = pipeline(\"fill-mask\", model=checkpoint, device=device)\n",
    "        self.weights = weights or self.default_weights\n",
    "        self.max_good_targets = max_good_targets\n",
    "        self.max_bad_targets = max_bad_targets\n",
    "        self.template = template\n",
    "\n",
    "    def __call__(self, game: Game, hint_for_red=True) -> Iterator[Hint]:\n",
    "        all_words = set(game.all_words)\n",
    "        teams = game.teams(red=hint_for_red)\n",
    "        word2score = {\n",
    "            word: getattr(self.weights, cat) for word, cat in zip(game.all_words, teams)\n",
    "        }\n",
    "\n",
    "        team, other_team = (\n",
    "            (game.red, game.blue) if hint_for_red else (game.blue, game.red)\n",
    "        )\n",
    "        subset_generator = lambda: self.candidate_subsets(\n",
    "            team, other_team, game.neutral\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            inputs = self.create_inputs(subset_generator, game, hint_for_red)\n",
    "            all_results = self.unmasker(inputs)\n",
    "            for results, subset in zip(all_results, subset_generator()):\n",
    "                subset_score = sum(word2score[word] for word in subset)\n",
    "                for result in results:\n",
    "                    # exclude tokens are contained in the game\n",
    "                    if result[\"token_str\"] in all_words:\n",
    "                        continue\n",
    "                    yield Hint(\n",
    "                        word=result[\"token_str\"],\n",
    "                        score=result[\"score\"] * subset_score,\n",
    "                        targets=subset,\n",
    "                    )\n",
    "\n",
    "    @staticmethod\n",
    "    def words2string(words: Sequence[str]):\n",
    "        return \", \".join(words[:-1]) + f\" and {words[-1]}\"\n",
    "\n",
    "    def create_inputs(\n",
    "        self,\n",
    "        subset_generator: Callable[[], SubsetGenerator],\n",
    "        game: Game,\n",
    "        hint_for_red: bool,\n",
    "    ) -> list[str]:\n",
    "        return [\n",
    "            self.template.format(self.words2string(subset))\n",
    "            for subset in subset_generator()\n",
    "        ]\n",
    "\n",
    "    def candidate_subsets(\n",
    "        self, team: set[str], other_team: set[str], neutral: set[str]\n",
    "    ) -> SubsetGenerator:\n",
    "        # subsets containing words of team\n",
    "        def all_good_targets():\n",
    "            for n_good_targets in range(2, min(self.max_good_targets, len(team)) + 1):\n",
    "                for combination in combinations(team, n_good_targets):\n",
    "                    yield combination\n",
    "\n",
    "        # subsets containing words of other team or neutral\n",
    "        def all_bad_targets():\n",
    "            for n_bad_targets in range(self.max_bad_targets):\n",
    "                for combination in combinations(\n",
    "                    other_team.union(neutral), n_bad_targets + 1\n",
    "                ):\n",
    "                    yield combination\n",
    "\n",
    "        for good_targets in all_good_targets():\n",
    "            yield good_targets\n",
    "            for bad_targets in all_bad_targets():\n",
    "                yield good_targets + bad_targets\n",
    "\n",
    "    def n_subsets(self, team: set[str], other_team: set[str], neutral: set[str]) -> int:\n",
    "        n_good = len(team)\n",
    "        n_bad = len(other_team.union(neutral))\n",
    "        good_combos = sum(\n",
    "            comb(n_good, k, exact=True)\n",
    "            for k in range(2, min(self.max_good_targets, n_good) + 1)\n",
    "        )\n",
    "        bad_combos = sum(\n",
    "            comb(n_bad, k, exact=True)\n",
    "            for k in range(1, min(self.max_bad_targets, n_bad) + 1)\n",
    "        )\n",
    "        return good_combos + good_combos * bad_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_strategy = MaskFilling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_strategy.print_top_k(game, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_strategy.print_top_k(game, 10, hint_for_red=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "These results actually show a deeper semantic understanding that Strategy 1. I had to experiment a bit with the template, to prevent grammatical terms or inflected forms of game words in the results, but I am quite content with the results.\n",
    "\n",
    "Unfortunately, the results vary a lot with the nondeterministic components, so you may get completely different hints.\n",
    "I liked the red hints:\n",
    "* `earth` targeting `date`, `light` and `wave`\n",
    "* `energy` targeting `bond`, `light` and `wave`\n",
    "* `head` targeting `cap`, `part`, `wave` and `trunk` (though trunk is neutral)\n",
    "\n",
    "and for blue:\n",
    "* `witness` targeting `trial`, `bill`, `stand` and `chest`\n",
    "* `truck` targeting `bill`, `chest`, `jam` and `plane`\n",
    "* `defense` targeting `trial`, `stand` and `chest`\n",
    "  \n",
    "As mentioned above, this strategy will give hints that also target other terms. For example, `energy` can also be associated with `plane` of the other team, in some sense.\n",
    "  \n",
    "In a refined version, this can be excluded by a more elaborate input that also takes into account the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Strategy 3: MaskFilling with exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only have to adapt the template and include the 'bad words' (assassin or other team)\n",
    "# The remaining behaviour is inherited from MaskFilling\n",
    "\n",
    "EXCLUSION_TEMPLATE = (\n",
    "    \"The word [MASK] can be thought of together with the words {included}, \"\n",
    "    + \"but does not relate to any of the words {excluded}.\"\n",
    ")\n",
    "\n",
    "\n",
    "class MaskFillingWithExclusion(MaskFilling):\n",
    "\n",
    "    def create_inputs(\n",
    "        self,\n",
    "        subset_generator: Callable[[], SubsetGenerator],\n",
    "        game: Game,\n",
    "        hint_for_red: bool,\n",
    "    ) -> list[str]:\n",
    "        inputs = []\n",
    "        bad_terms = set(game.blue if hint_for_red else game.red).union([game.assassin])\n",
    "        for included in subset_generator():\n",
    "            excluded = list(bad_terms.difference(included))\n",
    "            inputs.append(\n",
    "                self.template.format(\n",
    "                    included=self.words2string(included),\n",
    "                    excluded=self.words2string(excluded),\n",
    "                )\n",
    "            )\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_with_exclusion_strategy = MaskFillingWithExclusion(template=EXCLUSION_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_with_exclusion_strategy.print_top_k(game, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_with_exclusion_strategy.print_top_k(game, 25, hint_for_red=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* The MaskFilling Strategies can be configured:\n",
    "    * `max_good_targets`: how many targets of own team at max?\n",
    "    * `max_bad_targets`: how many neutral or words of the other team are allowed at max?\n",
    "    * `template`: change the masking template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
